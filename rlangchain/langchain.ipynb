{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3573257-313c-4728-af63-8da3612ed08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "790ec73a-e69d-4353-b5b5-63c22274c373",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "396e6686-b2b1-479e-8f54-6c33356d5366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "# Verbose is required to pass to the callback manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0bec3dd-cba7-4692-9a85-68c4f8368099",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/tonoy/Documents/programming/sidecar/rlangchain/rlangchain/models/llama-2-13b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: mem required  = 8819.71 MB (+ 1608.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/tonoy/Library/Caches/pypoetry/virtualenvs/rlangchain-ukarQTwn-py3.11/lib/python3.11/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x10d8b0840\n",
      "ggml_metal_init: loaded kernel_mul                            0x10d8af620\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x10d8b12c0\n",
      "ggml_metal_init: loaded kernel_scale                          0x10d8b1520\n",
      "ggml_metal_init: loaded kernel_silu                           0x11c3b3670\n",
      "ggml_metal_init: loaded kernel_relu                           0x10d8b1780\n",
      "ggml_metal_init: loaded kernel_gelu                           0x10d8b19e0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x11c3b38d0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x10df69d20\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x13df861e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x11c3b3b30\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x10d8b1c40\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x10d8b1ea0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x10d8b2100\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x10d8b2360\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x10d8b25c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x10d8b2820\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x11c3b3d90\n",
      "ggml_metal_init: loaded kernel_norm                           0x11c3b3ff0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x10d8b3120\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x10d8b2dc0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x10d8b3380\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x11c3b44e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x11c3b4c50\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x10d8b35e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x11c3b4eb0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x10d8b3840\n",
      "ggml_metal_init: loaded kernel_rope                           0x10d8b3aa0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x11c3b54b0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x11c3b5f70\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x10d8b4070\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x10d8b4a60\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 10922.67 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  6984.06 MB, ( 6984.52 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1026.00 MB, ( 8010.52 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, ( 8412.52 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   298.00 MB, ( 8710.52 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB, ( 9222.52 / 10922.67)\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/tonoy/Documents/programming/sidecar/rlangchain/rlangchain/models/llama-2-13b-chat.ggmlv3.q4_0.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96693ea8-7aa0-4fd8-849b-6d93f4307191",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deba9d77-2fe7-4aae-8c4c-03a33c5f0d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "1. Justin Bieber was born on March 1st, 1994.\n",
      "2. The Super Bowl is played at the end of the NFL season, which usually falls in early February.\n",
      "3. The NFL season starts in September and ends in early February (the week before the Super Bowl).\n",
      "\n",
      "With that in mind, let's look at the list of past Super Bowl winners to find out which team won the Super Bowl in 1994:\n",
      "\n",
      "Super Bowl Winners:\n",
      "\n",
      "* Super Bowl XXVII (1992): Dallas Cowboys\n",
      "* Super Bowl XXVIII (1993): Buffalo Bills\n",
      "* Super Bowl XXIX (1994): San Francisco 49ers\n",
      "\n",
      "Ah, we have our answer! The San Francisco 49ers won Super Bowl XXIX in 1994, the year Justin Bieber was born. "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 15444.40 ms\n",
      "llama_print_timings:      sample time =   150.48 ms /   211 runs   (    0.71 ms per token,  1402.20 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15444.19 ms /    45 tokens (  343.20 ms per token,     2.91 tokens per second)\n",
      "llama_print_timings:        eval time = 21955.74 ms /   210 runs   (  104.55 ms per token,     9.56 tokens per second)\n",
      "llama_print_timings:       total time = 37995.78 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" \\n\\n1. Justin Bieber was born on March 1st, 1994.\\n2. The Super Bowl is played at the end of the NFL season, which usually falls in early February.\\n3. The NFL season starts in September and ends in early February (the week before the Super Bowl).\\n\\nWith that in mind, let's look at the list of past Super Bowl winners to find out which team won the Super Bowl in 1994:\\n\\nSuper Bowl Winners:\\n\\n* Super Bowl XXVII (1992): Dallas Cowboys\\n* Super Bowl XXVIII (1993): Buffalo Bills\\n* Super Bowl XXIX (1994): San Francisco 49ers\\n\\nAh, we have our answer! The San Francisco 49ers won Super Bowl XXIX in 1994, the year Justin Bieber was born. \""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e9d63e8-34bd-4c43-b91e-d82e84ebd22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"write a python function that read data from a csv file and show th info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f36661f-da6e-41c8-a8a3-79b0bdfac6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "First, are you looking to simply read the contents of a CSV file into a list of lists or do you need to parse specific columns/fields within the CSV?\n",
      "\n",
      "If you just need to read the contents of the CSV file, then you can use the `csv` module in Python as follows:\n",
      "```\n",
      "import csv\n",
      "\n",
      "# Open the CSV file\n",
      "with open('data.csv', 'r') as f:\n",
      "    reader = csv.reader(f)\n",
      "    \n",
      "    # Print out each row\n",
      "    for row in reader:\n",
      "        print(row)\n",
      "```\n",
      "This will read the contents of the `data.csv` file and print out each row as a list of values, with each value separated by a comma.\n",
      "\n",
      "If you need to parse specific columns/fields within the CSV, then you can use the `csv.DictReader` class in the `csv` module to create a dictionary for each row, where the keys are the column headers and the values are the corresponding values for that header:\n",
      "```\n",
      "import csv\n",
      "\n",
      "# Open the CSV file\n",
      "with open('data.csv', 'r') as f:\n",
      "    reader = csv.DictReader(f)\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 15444.40 ms\n",
      "llama_print_timings:      sample time =   191.12 ms /   256 runs   (    0.75 ms per token,  1339.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4575.76 ms /    40 tokens (  114.39 ms per token,     8.74 tokens per second)\n",
      "llama_print_timings:        eval time = 16210.54 ms /   255 runs   (   63.57 ms per token,    15.73 tokens per second)\n",
      "llama_print_timings:       total time = 21436.66 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nFirst, are you looking to simply read the contents of a CSV file into a list of lists or do you need to parse specific columns/fields within the CSV?\\n\\nIf you just need to read the contents of the CSV file, then you can use the `csv` module in Python as follows:\\n```\\nimport csv\\n\\n# Open the CSV file\\nwith open('data.csv', 'r') as f:\\n    reader = csv.reader(f)\\n    \\n    # Print out each row\\n    for row in reader:\\n        print(row)\\n```\\nThis will read the contents of the `data.csv` file and print out each row as a list of values, with each value separated by a comma.\\n\\nIf you need to parse specific columns/fields within the CSV, then you can use the `csv.DictReader` class in the `csv` module to create a dictionary for each row, where the keys are the column headers and the values are the corresponding values for that header:\\n```\\nimport csv\\n\\n# Open the CSV file\\nwith open('data.csv', 'r') as f:\\n    reader = csv.DictReader(f)\\n    \\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a488f21-290d-448c-a6a3-5586bb02f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"I love you translate to thia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a97ec78-15cb-4be3-a5cd-618840e155cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are some possible translations for \"I love you\": \n",
      "* Italian - \"Ti amo\" (pronounced \"tee AH-moh\")\n",
      "* Spanish - \"Te quiero\" (pronounced \"teh KAY-yoh\")\n",
      "* French - \"Je t'aime\" (pronounced \"zhuh TAH-may\")\n",
      "* German - \"Ich liebe dich\" (pronounced \"ick lee-bah deekh\")\n",
      "* Portuguese - \"Eu te amo\" (pronounced \"yoo teh AH-moh\")\n",
      "Thai - \"ันจำแน่องค์กับคุณ\" (pronounced \"chan jum nai ing khon krup khun\")\n",
      "So, the translation for \"I love you\" in Thai is: \n",
      "ันจำแน่องค์กับคุณ\n",
      "(pronounced \"chan jum nai ing khon krup khun\")\n",
      "This can be shortened to:\n",
      "ันจำแน"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 15444.40 ms\n",
      "llama_print_timings:      sample time =   183.96 ms /   256 runs   (    0.72 ms per token,  1391.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3956.38 ms /    32 tokens (  123.64 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:        eval time = 16250.63 ms /   255 runs   (   63.73 ms per token,    15.69 tokens per second)\n",
      "llama_print_timings:       total time = 20843.21 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Here are some possible translations for \"I love you\": \\n* Italian - \"Ti amo\" (pronounced \"tee AH-moh\")\\n* Spanish - \"Te quiero\" (pronounced \"teh KAY-yoh\")\\n* French - \"Je t\\'aime\" (pronounced \"zhuh TAH-may\")\\n* German - \"Ich liebe dich\" (pronounced \"ick lee-bah deekh\")\\n* Portuguese - \"Eu te amo\" (pronounced \"yoo teh AH-moh\")\\nThai - \"ันจำแน่องค์กับคุณ\" (pronounced \"chan jum nai ing khon krup khun\")\\nSo, the translation for \"I love you\" in Thai is: \\nันจำแน่องค์กับคุณ\\n(pronounced \"chan jum nai ing khon krup khun\")\\nThis can be shortened to:\\nันจำแน'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.run(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
